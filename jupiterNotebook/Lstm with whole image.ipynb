{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df8dc66-5d85-4150-8d48-65237523b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Global Variables\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, TimeDistributed, LSTM, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Global variables\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_CHANNELS = 3\n",
    "BATCH_SIZE = 10\n",
    "TIMESTEPS = 24\n",
    "ECHPOS_SIZE=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542a1988-8560-46b7-ba95-328977baf641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10610 images\n"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=100000)\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image at {image_path}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    # Normalize pixel values to [0, 1] instead of VGG16 preprocessing\n",
    "    img_normalized = img_resized / 255.0\n",
    "    return img_normalized\n",
    "\n",
    "def load_images_with_data(image_dir=\"F:/Pheonix/Thesis/Project/Code/AQI Index/.venv/Dev/Data/cleaned_folder\",\n",
    "                         csv_file=\"F:/Pheonix/Thesis/Project/Code/AQI Index/.venv/Dev/Data/cleaned_data.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_data_list = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_name = row['Filename']\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                img_resized = load_and_preprocess_image(image_path)\n",
    "                values = row.to_dict()\n",
    "                values['image'] = img_resized\n",
    "                image_data_list.append(values)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {image_name}: {e}\")\n",
    "    return image_data_list\n",
    "\n",
    "image_data_list = load_images_with_data()\n",
    "print(f\"Loaded {len(image_data_list)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4418528c-cf9f-4a48-a00f-723a77a15008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shape: (443, 24, 32, 32, 3)\n",
      "Dataset shape: X=(443, 24, 32, 32, 3), y_dict sample shape: (443, 24)\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(image_data_list):\n",
    "    \"\"\"Prepare dataset with whole images, no segmentation.\"\"\"\n",
    "    X = []\n",
    "    y_dict = {'AQI': [], 'PM2.5': [], 'PM10': [], 'O3': [], 'CO': [], 'SO2': [], 'NO2': []}\n",
    "    sequence_X = []\n",
    "    scalers = {}\n",
    "    \n",
    "    for i, data in enumerate(image_data_list):\n",
    "        img = data['image']\n",
    "        if img.shape != (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS):\n",
    "            img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        \n",
    "        sequence_X.append(img)\n",
    "        for param in y_dict.keys():\n",
    "            y_dict[param].append(data[param])\n",
    "        \n",
    "        if (i + 1) % TIMESTEPS == 0:\n",
    "            X.append(np.array(sequence_X))\n",
    "            sequence_X = []\n",
    "    \n",
    "    if sequence_X:\n",
    "        remaining_steps = TIMESTEPS - len(sequence_X)\n",
    "        padding = np.zeros((remaining_steps, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float32)\n",
    "        sequence_X = np.concatenate([np.array(sequence_X), padding], axis=0)\n",
    "        X.append(sequence_X)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Normalize y values\n",
    "    y_dict_seq = {}\n",
    "    for param, values in y_dict.items():\n",
    "        sequences = [values[i:i+TIMESTEPS] for i in range(0, len(values), TIMESTEPS)]\n",
    "        if len(sequences[-1]) < TIMESTEPS:\n",
    "            sequences[-1] = np.pad(sequences[-1], (0, TIMESTEPS - len(sequences[-1])), mode='constant', constant_values=0)\n",
    "        sequences = np.array(sequences)\n",
    "        scaler = StandardScaler()\n",
    "        flat_values = sequences.reshape(-1, 1)\n",
    "        scaled_values = scaler.fit_transform(flat_values).reshape(sequences.shape)\n",
    "        y_dict_seq[param] = scaled_values\n",
    "        scalers[param] = scaler\n",
    "    \n",
    "    print(f\"Final X shape: {X.shape}\")\n",
    "    return X, y_dict_seq, scalers\n",
    "\n",
    "X, y_dict, scalers = prepare_dataset(image_data_list)\n",
    "print(f\"Dataset shape: X={X.shape}, y_dict sample shape: {y_dict['AQI'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cc7d94-07d1-4f68-8624-babfd3acd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 24, 32, 32, 32)   896       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 24, 16, 16, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 24, 16, 16, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 24, 8, 8, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 24, 4096)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                1065216   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,086,721\n",
      "Trainable params: 1,086,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_cnn_lstm_model(input_shape=(TIMESTEPS, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)):\n",
    "    \"\"\"CNN-LSTM for 32x32 whole images.\"\"\"\n",
    "    model = Sequential([\n",
    "        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=input_shape),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),  # 32x32 -> 16x16\n",
    "        TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),  # 16x16 -> 8x8\n",
    "        TimeDistributed(Flatten()),  # 8x8x64 = 4096 features per timestep\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_cnn_lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b14cf4-a3c9-47e0-aa63-72cb235c9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for AQI\n",
      "Training samples: 354, Testing samples: 89\n",
      "Epoch 1/150\n",
      "27/36 [=====================>........] - ETA: 12s - loss: 1.0443 - mae: 0.8438"
     ]
    }
   ],
   "source": [
    "def train_models(X, y_dict):\n",
    "    models = {}\n",
    "    histories = {}\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    for param, y_values in y_dict.items():\n",
    "        print(f\"\\nTraining model for {param}\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_values, test_size=0.2, random_state=42)\n",
    "        print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "        \n",
    "        model = build_cnn_lstm_model()\n",
    "        history = model.fit(\n",
    "            X_train, y_train[:, -1],\n",
    "            validation_data=(X_test, y_test[:, -1]),\n",
    "            epochs=ECHPOS_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        models[param] = model\n",
    "        histories[param] = history\n",
    "        \n",
    "        model_path = os.path.join(\"saved_models\", f\"model_{param}.h5\")\n",
    "        model.save(model_path)\n",
    "        print(f\"Saved model for {param} to {model_path}\")\n",
    "    \n",
    "    return models, histories\n",
    "\n",
    "models, histories = train_models(X, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2c528-2316-42d7-bd39-842f1c58cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for param, history in histories.items():\n",
    "    plt.plot(history.history['mae'], label=f'{param} Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label=f'{param} Val MAE', linestyle='--')\n",
    "plt.title('Training and Validation MAE Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dcded-7861-4465-b8bb-4bada9c5b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(models, X_test=None, y_test_dict=None, scalers=None, new_image_path=None):\n",
    "    results = {}\n",
    "    \n",
    "    if X_test is not None and y_test_dict is not None:\n",
    "        print(f\"\\nEvaluating on test data (sequences: {X_test.shape[0]})\")\n",
    "        for param, model in models.items():\n",
    "            test_loss, test_mae = model.evaluate(X_test, y_test_dict[param][:, -1], verbose=0)\n",
    "            y_pred_scaled = model.predict(X_test, verbose=0).flatten()\n",
    "            y_pred = scalers[param].inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "            y_true = scalers[param].inverse_transform(y_test_dict[param][:, -1].reshape(-1, 1)).flatten()\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            results[param] = {'MSE': test_loss, 'MAE': test_mae, 'R²': r2}\n",
    "            print(f\"{param} - Test MSE (scaled): {test_loss:.4f}, Test MAE (scaled): {test_mae:.4f}, R²: {r2:.4f}\")\n",
    "            print(f\"Sample predictions (unscaled): {y_pred[:5]}, Actual: {y_true[:5]}\")\n",
    "    \n",
    "    elif new_image_path is not None:\n",
    "        print(\"\\nPredicting for a new sequence:\")\n",
    "        img_resized = load_and_preprocess_image(new_image_path)\n",
    "        sky_img_seq = np.expand_dims(img_resized, axis=0)\n",
    "        sky_img_seq = np.repeat(sky_img_seq[np.newaxis, :], TIMESTEPS, axis=1)\n",
    "        \n",
    "        for param, model in models.items():\n",
    "            pred_scaled = model.predict(sky_img_seq, verbose=0)[0]\n",
    "            pred = scalers[param].inverse_transform([[pred_scaled]])[0][0]\n",
    "            results[param] = pred\n",
    "            print(f\"Predicted {param}: {pred:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "X_train, X_test, y_train_dict, y_test_dict = {}, {}, {}, {}\n",
    "for param, y_values in y_dict.items():\n",
    "    X_train[param], X_test[param], y_train_dict[param], y_test_dict[param] = train_test_split(\n",
    "        X, y_values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "test_results = test_model(models, X_test['AQI'], y_test_dict, scalers)\n",
    "new_image = \"F:/Pheonix/Thesis/Project/Code/AQI Index/.venv/Dev/Data/test_image.jpg\"\n",
    "pred_results = test_model(models, new_image_path=new_image, scalers=scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8a45d-1226-4c60-9825-773aad188707",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_and_preprocess_image(new_image)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.suptitle(\"Sample Image with Predictions:\\n\" + \n",
    "             \"\\n\".join(f\"{param}: {value:.2f}\" for param, value in pred_results.items()))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
